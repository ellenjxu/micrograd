{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3262, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 27]),\n",
       " torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([27, 64]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help find grads for matmul\n",
    "W2.shape, dlogits.shape, h.shape, torch.transpose(W2, 0, 1).shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "bndiff          | exact: False | approximate: False | maxdiff: 0.0010958157945424318\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dloss = 1\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n # loss = -sum(correct log probs)/32, so -1/32 is derivative for all the correct log probs, 0 for others\n",
    "dprobs = 1.0/probs * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts # counts = norm_logits.exp()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) # ~0 since it is only there for numerical stability\n",
    "dlogits = dnorm_logits.clone()  # or torch.ones_like(dlogits) * dnorm_logits\n",
    "temp = torch.zeros_like(logits)\n",
    "temp[range(n), logits.max(1).indices] = 1.0\n",
    "dlogits += temp * dlogit_maxes  # or `F.out_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar = -0.5*(bnvar + 1e-5)**-1.5 * dbnvar_inv # (bnvar + 1e-5)**-0.5\n",
    "dbndiff2 = torch.ones_like(bndiff2)/(n-1) * dbnvar  # 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "dbndiff += 2.0*bndiff * dbndiff2\n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "dhprebn = dbndiff\n",
    "dhprebn += torch.ones_like(hprebn)/n* dbnmeani   # 1/n*hprebn.sum(0, keepdim=True)\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "demb = dembcat.view(emb.shape)  # (32, 3, 10)\n",
    "dC = torch.zeros_like(C)\n",
    "## This probably didn't work because duplicates may only be indexed once, ex [1,1,4]\n",
    "# for i in range(n):\n",
    "#     dC[Xb[i]] += demb[i]\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        dC[Xb[i,j]] += demb[i,j]\n",
    "\n",
    "# -----------------\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3377387523651123 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n",
    "# note: F.cross_entropy is computing logits (activations from nn) => softmax to get probs => calculate nll -log(probs) and averaging over all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd75a7a6970>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show each Yb \"plucking out\" an index of logits which is the correct answer\n",
    "plt.imshow(F.one_hot(Yb, num_classes=logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "\n",
    "## same as softmax\n",
    "# dlogits = logits.exp()\n",
    "# lsum = dlogits.sum(1, keepdim=True)\n",
    "# dlogits = dlogits/lsum\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1.0\n",
    "dlogits /= n\n",
    "# note: derivative if logit is not correct answer is P_i and for correct answer is P_i-1, where P is softmax probability\n",
    "# this makes sense because if P_i is 1 (i.e. correct answer is predicted fully correct), then P_i-1=0, no need to update; same for not correct P_i=0 no need to update\n",
    "# grad is proportional to the amount you mispredict! Think of it as subtracting correct label (0 or 1) from prob to see how much it is wrong\n",
    "\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd75b02f340>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAKTCAYAAADlpSlWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAviElEQVR4nO3dXYxc9Xk/8Gd2dnd2ba+duMRebzCuk5g0iQlSIAWsJDioWPgCJSGVSJEio7ZREC8SsqK0hItYVWWnVEGpREOVXFBQQ8NF8yZBIa4IJhElBRQaagK1sYmdxo6FAa93va8z53/hP6sseIG1n2WWnz8faSTvzPg7z5w558x3z+6eqVVVVQUAQCE62j0AAEAm5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFE62z3Aq7Varfjtb38bfX19UavV2j0OADAPVFUVR48ejYGBgejoeP1jM/Ou3Pz2t7+NlStXtnsMAGAe2r9/f5x55pmve595V276+voiIuK///u/p/59Kt6o3c3G4OBgWlZERHd3d1rW+Ph4WlbGcv99Q0NDaVn1ej0t64Mf/GBa1s6dO9OyImLeHrXMPKF59snRM5fZ5ORkWlbm88zcn0XkztbT05OWlSlz35ht4cKFaVnNZjMta3R0NC0rIm89GxoainXr1r2p96h5V25e2UH19fWlvMlmvhlm74zna7lZvHhxWlZE7ptO5uuZKbsQKjezp9zMnnLTXvO13HR1daVlRbRnW/cLxQBAUZQbAKAoyg0AUJQ5Kzff/OY3Y/Xq1dHT0xPnnXde/PSnP52rhwIAmDIn5eaee+6JG2+8MW6++eb4xS9+ER//+Mdj48aNsW/fvrl4OACAKXNSbm699db4i7/4i/jLv/zL+MAHPhDf+MY3YuXKlXH77bfPxcMBAExJLzfj4+PxxBNPxIYNG6Zdv2HDhnjkkUdec/+xsbEYHBycdgEAOFnp5eaFF16IZrMZy5cvn3b98uXL4+DBg6+5/7Zt22LJkiVTF2cnBgBOxZz9QvGrT7JTVdUJT7xz0003xZEjR6Yu+/fvn6uRAIDTQPoZis8444yo1+uvOUpz6NCh1xzNiYhoNBrRaDSyxwAATlPpR266u7vjvPPOi+3bt0+7fvv27bFu3brshwMAmGZOPltq8+bN8fnPfz7OP//8uOiii+Jb3/pW7Nu3L6655pq5eDgAgClzUm6uvPLKOHz4cPzN3/xNHDhwINauXRv33XdfrFq1ai4eDgBgypx9Kvi1114b11577VzFAwCckM+WAgCKotwAAEWZsx9LnarJycmYnJxMycnyzne+My0rImJkZCQtq16vp2UNDQ2lZUUcP8dRlo6OvD6+d+/etKxWq5WWFXH8rw6zNJvNtKwTnavqZGWuFxERa9asScvavXt3WlbmupH5Wkbkvp6Z+9r5us5G5M6WuQ2Mjo6mZWXuZyPynuds5nLkBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABSls90DzGR0dDS6urpOOadWqyVMc9zIyEhaVraOjryeWq/X07IiIrq7u9OyMl/PBQsWpGWNjY2lZWXnZb6emcu/szN39/PMM8+kZa1atSota/fu3WlZ2cusqqq0rHe84x1pWZn72vHx8bSsiNztab5u55OTk2lZEbnvT2/6Md/yRwQAmEPKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGAChKZ7sHmEm9Xo96vX7KOa1WK2Ga47q6utKyIiI6O/MWf0dHXk8dGRlJy5rPMtavVzSbzbSsiNx1I3O2zPUsc9uMiOjt7U3LOnDgQFpW5vaUvcwy844cOZKWNTk5mZZVq9XSsiIi3ve+96Vl7d69Oy0r83l2d3enZWWazX7RkRsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQlM52DzCTD33oQyk5e/bsScmZC81mMy2rqqq0rM7O3NUic7bJycm0rK6urrSs+bzMWq1WWla9Xk/Lylz/IyI6OvK+VxsYGEjLev7559OyGo1GWla2Wq3W7hFOaGJiIjVv165daVmZ22bmupG9zLL2G7NZXo7cAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKJ0tnuAmTz99NPR19d3yjlVVSVMc1x3d3daVkRErVabl1nHjh1Ly4rIna3RaKRljY2NpWU1m820rIiIrq6u1Lwsmc+zXq+nZUVEdHbm7c4OHDiQlpUpc52NiGi1WmlZa9asSct6/vnn07Ky17OOjrxjApOTk2lZmevG4sWL07Ii8mabzbJ35AYAKIpyAwAURbkBAIqi3AAARVFuAICipJebLVu2RK1Wm3bp7+/PfhgAgBOakz8F/9CHPhT/8R//MfV19p/iAQDMZE7KTWdnp6M1AEBbzMnv3OzatSsGBgZi9erV8bnPfS727Nkz433HxsZicHBw2gUA4GSll5sLLrgg7rrrrnjggQfi29/+dhw8eDDWrVsXhw8fPuH9t23bFkuWLJm6rFy5MnskAOA0kl5uNm7cGJ/97GfjnHPOiT/5kz+Je++9NyIi7rzzzhPe/6abboojR45MXfbv3589EgBwGpnzz5ZauHBhnHPOObFr164T3t5oNFI/LwgAOL3N+XluxsbG4le/+lWsWLFirh8KACC/3HzpS1+KHTt2xN69e+PnP/95/Omf/mkMDg7Gpk2bsh8KAOA10n8s9Zvf/Cb+7M/+LF544YV417veFRdeeGE8+uijsWrVquyHAgB4jfRy893vfjc7EgDgTfPZUgBAUZQbAKAoc/6n4CerXq+nfCbVyMhIwjTH9fT0pGVFRAwNDaVldXbmvZRVVaVlReQut1arlZbV1dWVlnX22WenZUVEPPPMM2lZmetG5vKfmJhIy8rOW7RoUVpW5qkujh07lpYVETE+Pp6WtXfv3rSszPWsu7s7LSsiotlspmXVarW0rMztPPO9KSLveU5OTr7p+zpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARels9wAzqaoqqqo65ZzOzryneOzYsbSsiIh3vetdaVkvvvhiWlaj0UjLiogYGxtLy1q0aFFaVubruXPnzrSsiIiOjrzvOyYnJ9OyarVaWlZPT09aVkTEwMBAWtaePXvSsuazzNdz8eLFaVlDQ0NpWdmazWZaVr1eT8vKnCv7PSBrHzSb/aIjNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAone0e4O2k1Wql5r388stpWZOTk2lZa9asScuKiHj++edT87Jkvp71ej0tK1vmbB0ded8PjY+Pp2VFRDz33HNpWbVaLS0rU2dn7i672WymZWXvH7P09PSk5mUus8ztKdPIyEhqXtZ6W1XVm77v/FyyAAAnSbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIrS2e4BZjIxMRETExOnnPOHf/iHpz7M//frX/86LSsiYnJyMi2rq6srLWv37t1pWRG5zzNjnXjFO97xjrSs0dHRtKyIiOHh4bSszHUjU71eb/cIM6rVamlZjUYjLavZbKZlZTt69Gha1oIFC9KyBgcH07IiInp6etKyRkZG0rIyt6fsfUbWe8Bs1n9HbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBROts9wExarVa0Wq1Tztm9e3fCNMd1dOR2wcy8qqrSsrJNTk6mZWWsE68YHBxMy6rX62lZEbnrRuby7+npScsaHx9Py4rIfQ2WL1+elvXCCy+kZdVqtbSsiIgFCxakZQ0PD6dlrVy5Mi3r6aefTsuKyH2emets5rqRuZ+NyJttNjmO3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKMqsy83DDz8cl19+eQwMDEStVosf/OAH026vqiq2bNkSAwMD0dvbG+vXr4+dO3dmzQsA8LpmXW6Gh4fj3HPPjdtuu+2Et99yyy1x6623xm233RaPPfZY9Pf3x6WXXhpHjx495WEBAN7IrE/it3Hjxti4ceMJb6uqKr7xjW/EzTffHFdccUVERNx5552xfPnyuPvuu+OLX/zia/7P2NhYjI2NTX2deWI1AOD0k/o7N3v37o2DBw/Ghg0bpq5rNBpx8cUXxyOPPHLC/7Nt27ZYsmTJ1CXzzJQAwOkntdwcPHgwIl576vLly5dP3fZqN910Uxw5cmTqsn///syRAIDTzJx8ttSrP/+hqqoZPxOi0WhEo9GYizEAgNNQ6pGb/v7+iIjXHKU5dOhQ6gfRAQDMJLXcrF69Ovr7+2P79u1T142Pj8eOHTti3bp1mQ8FAHBCs/6x1NDQUOzevXvq671798aTTz4ZS5cujbPOOituvPHG2Lp1a6xZsybWrFkTW7dujQULFsRVV12VOjgAwInMutw8/vjj8clPfnLq682bN0dExKZNm+Kf//mf48tf/nKMjIzEtddeGy+99FJccMEF8eMf/zj6+vrypgYAmMGsy8369eujqqoZb6/VarFly5bYsmXLqcwFAHBSfLYUAFAU5QYAKMqcnOcmQ0dHR3R0nHr3qtfrCdMc12w207IiIi677LK0rHvvvTcta8GCBWlZEZF6HqPx8fG0rEyTk5Opea1WKy1rpnNMnYzR0dG0rMy5ImLax7icqn379qVlZe6DOjtzd9lHjhxJy+rp6UnL2rt3b1pW9n57YmIiLStz3cjMynjv/X1Z2+Zs9ouO3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICidLZ7gJm0Wq1otVrtHmOaRqORmnffffelZdXr9bSskZGRtKyIiEWLFqVlVVWVlvWBD3wgLWvXrl1pWRERzWYzLauzc35u5tnbd0dH3vdqXV1daVmZ+42JiYm0rIjc5zk+Pp6W1d3dnZaV7Z3vfGda1osvvpiWlfkekC1rttnkOHIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAitLZ7gFmUqvVolarpeRk6ejI7YKZs7VarbSsvr6+tKyIiOHh4bSszOf59NNPp2VVVZWWFZG7rmXO1mg00rLGx8fTsiIi3v/+96dlPffcc2lZIyMjaVmZ+4yIiIULF6Zlvfzyy2lZ9Xo9LWt0dDQtKyLipZdeSsvq7u5Oy5rPsvZBs1n/HbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARels9wAz6e7uju7u7lPOmZiYSJjmuPHx8bSsiIienp60rJGRkXmZFRFRq9XSsnp7e9OyMlVVlZqXucwys84666y0rN27d6dlRUQ888wzaVmZ+43MdaPRaKRlRUQMDQ2lZWXuz5rNZlpW9j5jbGwsNS9L5jKbr/uzVqv1pu/ryA0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABSls90DzGTt2rVRq9VOOWffvn0J0xw3MTGRlhURMTo6mpaVsaxesXDhwrSsiIihoaG0rLGxsbSszGXW3d2dljWf/frXv07LOnbsWFpWRES9Xk/LajabaVldXV1pWZnrf0REb29vWlbm65m5zDJfy4iIjo68YwKNRiMtq6qqtKzM96aIiFarlZIzm+foyA0AUBTlBgAoinIDABRFuQEAiqLcAABFmXW5efjhh+Pyyy+PgYGBqNVq8YMf/GDa7VdffXXUarVplwsvvDBrXgCA1zXrcjM8PBznnntu3HbbbTPe57LLLosDBw5MXe67775TGhIA4M2a9XluNm7cGBs3bnzd+zQajejv7z/poQAATtac/M7NQw89FMuWLYuzzz47vvCFL8ShQ4dmvO/Y2FgMDg5OuwAAnKz0crNx48b4zne+Ew8++GB8/etfj8ceeywuueSSGc+suW3btliyZMnUZeXKldkjAQCnkfSPX7jyyiun/r127do4//zzY9WqVXHvvffGFVdc8Zr733TTTbF58+aprwcHBxUcAOCkzflnS61YsSJWrVoVu3btOuHtjUYj9fM1AIDT25yf5+bw4cOxf//+WLFixVw/FADA7I/cDA0Nxe7du6e+3rt3bzz55JOxdOnSWLp0aWzZsiU++9nPxooVK+L555+Pr3zlK3HGGWfEZz7zmdTBAQBOZNbl5vHHH49PfvKTU1+/8vsymzZtittvvz2eeuqpuOuuu+Lll1+OFStWxCc/+cm45557oq+vL29qAIAZzLrcrF+/PqqqmvH2Bx544JQGAgA4FT5bCgAoinIDABRlzv8U/GQ9+eSTKb+nM9PJA0/G4sWL07Iijn9OV5bu7u60rNHR0bSsiIhms5mW1dGR18dbrVZaVvYyyzw9wrvf/e60rH379qVl9fT0pGVFRHR2zs/d2dDQULtHmFHmetvV1ZWWlbltZmZF5O7P6vV6WtbExERaVuZcEXnvT7N5jo7cAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKJ0tnuAmZx33nlRq9VOOec3v/lNwjTHjYyMpGVFRHR25i3+8fHxtKyqqtKyIiLldXzFwoUL07KGh4fTslqtVlpWRERXV1da1u7du9Oyms1mWtbk5GRaVkTubNnbQJZ6vZ6al7nMMtfZzHWjp6cnLSsid1+bmZW5n83W0ZFzHGU2OY7cAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKJ0tnuAmfzXf/1X9PX1nXLO4OBgwjTHNRqNtKyIiJGRkbSser2eltVsNtOyIiIWLVqUlpW5zHp7e9OyspfZ0NBQWlZ3d3daVqZWq5WaNzExkZbV1dWVlpW5/mc+x4iIjo68728zZ8vc12Zvm0uWLEnLevHFF9Oy5vN7wMDAQEpOVVVv+r6O3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFE62z3ATDo6OqKj49S7V1VVCdMc12w207KyZSyrV9RqtbSsiNzXoF6vp2WNj4+nZb3nPe9Jy4qI2L17d2pels7OvF1G5noRkbt9ZmaNjIykZbVarbSsiNztadGiRWlZY2NjaVnZ69nQ0FBa1oIFC9KyJiYm0rKy17PnnnsuJefo0aNxzjnnvKn7OnIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAitLZ7gFm0mg0otFonHLOyMhIwjTHNZvNtKyIiK6urrSsqqrSsmq1WlpWRO5r0NGR18czl/+uXbvSsiIient707JGR0fTsjKNjY2l5mXsL+Yia3BwMC0re9vMzBsfH5+XWZn7jIjc94FWq5WWlfk8P/CBD6RlRUT87//+b0pOvV5/0/d15AYAKIpyAwAURbkBAIqi3AAARVFuAICizKrcbNu2LT760Y9GX19fLFu2LD796U/Hs88+O+0+VVXFli1bYmBgIHp7e2P9+vWxc+fO1KEBAGYyq3KzY8eOuO666+LRRx+N7du3x+TkZGzYsCGGh4en7nPLLbfErbfeGrfddls89thj0d/fH5deemkcPXo0fXgAgFeb1Xlu7r///mlf33HHHbFs2bJ44okn4hOf+ERUVRXf+MY34uabb44rrrgiIiLuvPPOWL58edx9993xxS9+MW9yAIATOKXfuTly5EhERCxdujQiIvbu3RsHDx6MDRs2TN2n0WjExRdfHI888sgJM8bGxmJwcHDaBQDgZJ10uamqKjZv3hwf+9jHYu3atRERcfDgwYiIWL58+bT7Ll++fOq2V9u2bVssWbJk6rJy5cqTHQkA4OTLzfXXXx+//OUv41//9V9fc9urT+ldVdWMp/m+6aab4siRI1OX/fv3n+xIAAAn99lSN9xwQ/zoRz+Khx9+OM4888yp6/v7+yPi+BGcFStWTF1/6NCh1xzNeUXWZ0gBAETM8shNVVVx/fXXx/e+97148MEHY/Xq1dNuX716dfT398f27dunrhsfH48dO3bEunXrciYGAHgdszpyc91118Xdd98dP/zhD6Ovr2/q92iWLFkSvb29UavV4sYbb4ytW7fGmjVrYs2aNbF169ZYsGBBXHXVVXPyBAAAft+sys3tt98eERHr16+fdv0dd9wRV199dUREfPnLX46RkZG49tpr46WXXooLLrggfvzjH0dfX1/KwAAAr2dW5aaqqje8T61Wiy1btsSWLVtOdiYAgJPms6UAgKIoNwBAUU7qT8HfCh/+8IdnPDfObDz//POnPsz/12w207KyTUxMpGV1d3enZUVEtFqttKyMdeIVY2NjaVlv5ke2s5G5rmUu/8xlVq/X07Ii5u/zzFxns5fZ5ORkWtaCBQvSskZGRtKyspdZ5raeuW5keuaZZ9o9wilz5AYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAUpbPdA8zk5z//efT19Z1yzvLlyxOmOe7//u//0rIiIsbGxtKy6vV6WtaxY8fSsiIiFi1alJY1OjqaltXb25uW1Ww207Iicp9nZ+f83MxbrVZq3uTkZFpWV1dXWtbChQvTsiYmJtKyIiJqtVpa1tGjR9Oy5vO2+c53vjMt68UXX0zLynwPyJa1rc8mx5EbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoSme7B5hJo9GIRqNxyjm1Wi1hmuMmJibSsiIiqqpKy+rp6UnLGhkZScuKiGi1WmlZmctsdHQ0LauzM3dTqtfrqXlZMpd/tq6urrSsjo687/vm8z4ocz3L3M7HxsbSsrJlrhuZWZnvAdnLv9lsvuU5jtwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAonS2e4CZNJvNaDabp5xz6NChhGmOGx4eTsuKiOjp6UnLGh0dTcvKnCsid7b3vve9aVl79uxJy5qcnEzLioh4xzvekZZ1+PDhtKyOjrzvhzK279/X3d2dljU+Pj4vs6qqSsuKyH0N6vV6Wlar1UrLylxnIyJeeOGFtKz3vOc9aVkHDhxIy8pez7LeUyYmJt70fR25AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEXpbPcAM2k0GtFoNE4559ixYwnTHNdqtdKyIiLGx8fTsur1elpWZ2fuapE52549e9KyMnV05H6fcOTIkbSsnp6etKyqqtKyarVaWlZExOTkZFpW5rbe1dWVltVsNtOyIiLWrl2blvXUU0+lZWXug7L32wsXLkzL+t3vfpeWNZ/Xs5GRkbc8x5EbAKAoyg0AUBTlBgAoinIDABRFuQEAijKrcrNt27b46Ec/Gn19fbFs2bL49Kc/Hc8+++y0+1x99dVRq9WmXS688MLUoQEAZjKrcrNjx4647rrr4tFHH43t27fH5ORkbNiwIYaHh6fd77LLLosDBw5MXe67777UoQEAZjKrkwncf//9076+4447YtmyZfHEE0/EJz7xianrG41G9Pf350wIADALp/Q7N6+caGzp0qXTrn/ooYdi2bJlcfbZZ8cXvvCFOHTo0IwZY2NjMTg4OO0CAHCyTrrcVFUVmzdvjo997GPTznK5cePG+M53vhMPPvhgfP3rX4/HHnssLrnkkhgbGzthzrZt22LJkiVTl5UrV57sSAAAJ//xC9dff3388pe/jJ/97GfTrr/yyiun/r127do4//zzY9WqVXHvvffGFVdc8Zqcm266KTZv3jz19eDgoIIDAJy0kyo3N9xwQ/zoRz+Khx9+OM4888zXve+KFSti1apVsWvXrhPenvUZUgAAEbMsN1VVxQ033BDf//7346GHHorVq1e/4f85fPhw7N+/P1asWHHSQwIAvFmz+p2b6667Lv7lX/4l7r777ujr64uDBw/GwYMHpz6pc2hoKL70pS/Ff/7nf8bzzz8fDz30UFx++eVxxhlnxGc+85k5eQIAAL9vVkdubr/99oiIWL9+/bTr77jjjrj66qujXq/HU089FXfddVe8/PLLsWLFivjkJz8Z99xzT/T19aUNDQAwk1n/WOr19Pb2xgMPPHBKAwEAnAqfLQUAFEW5AQCKctLnuZlr4+PjMT4+fso5b/SjtNmo1WppWRERrVYrLSvzz+mPHj2alhURsXjx4rSsY8eOpWVlrhtnn312WlZExNNPP52Wlbne1uv1tKxsHR3z83u1rq6utKzMdTYi4plnnknLypwtM6uzM/dtLnN/9rvf/S4tK/N5Zr43tcv83BsAAJwk5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAUpbPdA8yk2WxGs9ls9xjTNBqN1Lx3v/vdaVn79u1Ly6rVamlZERHDw8NpWVVVpWV1dOR1+z179qRlRUSMjo6mZbVarbSsTNnrWb1eT8vq7MzbNY6Pj6dlZa6z2XljY2NpWX/wB3+QlnX48OG0rOy8zG1zcnIyLStz/Y+I6OnpScmZmJh40/d15AYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAUpbPdA8ykt7c3ent7TzlnfHw8YZrjRkdH07IiIvbs2ZOal+WDH/xgat6zzz6bllWr1dKyMteNVquVlhUR0dmZt2lmztZsNtOyqqpKy8rOm5ycTMvK2I+94tixY2lZERHd3d1pWZnb05EjR9KyMrelbIsWLUrLynwtX3zxxbSsiLz99tjY2Ju+ryM3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCjKDQBQFOUGACiKcgMAFEW5AQCKotwAAEVRbgCAoig3AEBRlBsAoCid7R5gJseOHYt6vX7KOVVVJUxzXGfnvF1cqbPt3LkzLSsioqurKy1rbGwsLWvx4sVpWStWrEjLioh47rnn0rJqtVpaVqaOjvn7vVVPT09a1sjISFpW9ms5Pj6elpU5W8a+/xWTk5NpWRG5+9rh4eG0rMx9Y29vb1pWRESz2UzJmc17yfzduwAAnATlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoinIDABRFuQEAiqLcAABFUW4AgKIoNwBAUZQbAKAoyg0AUBTlBgAoSme7B5jJRz7ykajVaqec8+tf/zphmuPGx8fTsiIient707KazWZaVnd3d1pWRP5yy3Ls2LG0rF27dqVlRUR0dOR93zE5OZmWVVVVWlbmc4zI3QYyn2fGfuwVmXNF5L4G8zUr29jYWFrW4sWL07Iyl9mRI0fSsiLytoHZbOPzdw0CADgJyg0AUBTlBgAoinIDABRFuQEAiqLcAABFmVW5uf322+PDH/5wLF68OBYvXhwXXXRR/Pu///vU7VVVxZYtW2JgYCB6e3tj/fr1sXPnzvShAQBmMqtyc+aZZ8bXvva1ePzxx+Pxxx+PSy65JD71qU9NFZhbbrklbr311rjtttvisccei/7+/rj00kvj6NGjczI8AMCr1apTPCvU0qVL4+///u/jz//8z2NgYCBuvPHG+Ku/+quIOH6yo+XLl8ff/d3fxRe/+MUT/v+xsbFpJ0UaHByMlStXRmdnp5P4zcJ8PYFZxPyeLUur1UrN6+zMO7+mk/jNXubyn88n8ct8npky143s/Xbmtt7X15eWdTqcxO/o0aNxzjnnxJEjR97wBIgnvTSazWZ897vfjeHh4bjoooti7969cfDgwdiwYcPUfRqNRlx88cXxyCOPzJizbdu2WLJkydRl5cqVJzsSAMDsy81TTz0VixYtikajEddcc018//vfjw9+8INx8ODBiIhYvnz5tPsvX7586rYTuemmm+LIkSNTl/379892JACAKbM+Jvn+978/nnzyyXj55Zfj3/7t32LTpk2xY8eOqdtfffipqqrXPSTVaDSi0WjMdgwAgBOa9ZGb7u7ueN/73hfnn39+bNu2Lc4999z4h3/4h+jv74+IeM1RmkOHDr3maA4AwFw55d9AqqoqxsbGYvXq1dHf3x/bt2+fum18fDx27NgR69atO9WHAQB4U2b1Y6mvfOUrsXHjxli5cmUcPXo0vvvd78ZDDz0U999/f9Rqtbjxxhtj69atsWbNmlizZk1s3bo1FixYEFddddVczQ8AMM2sys3vfve7+PznPx8HDhyIJUuWxIc//OG4//7749JLL42IiC9/+csxMjIS1157bbz00ktxwQUXxI9//OPUP3cDAHg9p3yem2yDg4OxZMkS57mZpfl8Lpn5PFsW57mZPee5mT3nuZk957mZvdP6PDcAAPORcgMAFGV+HpOMiP/5n/9JOWSXeUgy88dIERHHjh1Ly8o8vDk8PJyWFZH744LMQ6+Zh5ez143M9Tb7xz9ZFixYkJo3OjqampelXq+nZWVuSxER73nPe9Kynn766bSshQsXpmVl/lg2Ine9HRoaSsvK/JFl9o8rs16D2TzH+bnXAwA4ScoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFGUGwCgKJ3tHuDVqqqKiIihoaGUvImJiZSciIjJycm0rIiIY8eOpeZlGR4eTs1rNptpWR0deX281WqlZWWvG+Pj46l581HmehERMTo6mpqXpV6vp2VlL7NX9rcZjh49mpaVuW3O5/3Z2NhYWlbma9nZmVsNsvaPr/SCN/Nca1XmEknwm9/8JlauXNnuMQCAeWj//v1x5plnvu595l25abVa8dvf/jb6+vqiVqvNeL/BwcFYuXJl7N+/PxYvXvwWTkiE5T8feA3ay/JvL8u/vdqx/KuqiqNHj8bAwMAbHsWfdz+W6ujoeMNG9vsWL15sxW4jy7/9vAbtZfm3l+XfXm/18l+yZMmbup9fKAYAiqLcAABFeduWm0ajEV/96lej0Wi0e5TTkuXffl6D9rL828vyb6/5vvzn3S8UAwCcirftkRsAgBNRbgCAoig3AEBRlBsAoCjKDQBQlLdtufnmN78Zq1evjp6enjjvvPPipz/9abtHOi1s2bIlarXatEt/f3+7xyrWww8/HJdffnkMDAxErVaLH/zgB9Nur6oqtmzZEgMDA9Hb2xvr16+PnTt3tmfYQr3Ra3D11Ve/Zpu48MIL2zNsYbZt2xYf/ehHo6+vL5YtWxaf/vSn49lnn512H9vA3Hkzy3++rv9vy3Jzzz33xI033hg333xz/OIXv4iPf/zjsXHjxti3b1+7RzstfOhDH4oDBw5MXZ566ql2j1Ss4eHhOPfcc+O222474e233HJL3HrrrXHbbbfFY489Fv39/XHppZemfkLz6e6NXoOIiMsuu2zaNnHfffe9hROWa8eOHXHdddfFo48+Gtu3b4/JycnYsGHDtE/6tg3MnTez/CPm6fpfvQ398R//cXXNNddMu+6P/uiPqr/+679u00Snj69+9avVueee2+4xTksRUX3/+9+f+rrValX9/f3V1772tanrRkdHqyVLllT/9E//1IYJy/fq16CqqmrTpk3Vpz71qbbMc7o5dOhQFRHVjh07qqqyDbzVXr38q2r+rv9vuyM34+Pj8cQTT8SGDRumXb9hw4Z45JFH2jTV6WXXrl0xMDAQq1evjs997nOxZ8+edo90Wtq7d28cPHhw2rbQaDTi4osvti28xR566KFYtmxZnH322fGFL3whDh061O6RinTkyJGIiFi6dGlE2Abeaq9e/q+Yj+v/267cvPDCC9FsNmP58uXTrl++fHkcPHiwTVOdPi644IK466674oEHHohvf/vbcfDgwVi3bl0cPny43aOddl5Z320L7bVx48b4zne+Ew8++GB8/etfj8ceeywuueSSGBsba/doRamqKjZv3hwf+9jHYu3atRFhG3grnWj5R8zf9b+zrY9+Cmq12rSvq6p6zXXk27hx49S/zznnnLjooovive99b9x5552xefPmNk52+rIttNeVV1459e+1a9fG+eefH6tWrYp77703rrjiijZOVpbrr78+fvnLX8bPfvaz19xmG5h7My3/+br+v+2O3JxxxhlRr9df08oPHTr0mvbO3Fu4cGGcc845sWvXrnaPctp55a/UbAvzy4oVK2LVqlW2iUQ33HBD/OhHP4qf/OQnceaZZ05dbxt4a8y0/E9kvqz/b7ty093dHeedd15s37592vXbt2+PdevWtWmq09fY2Fj86le/ihUrVrR7lNPO6tWro7+/f9q2MD4+Hjt27LAttNHhw4dj//79tokEVVXF9ddfH9/73vfiwQcfjNWrV0+73TYwt95o+Z/IfFn/35Y/ltq8eXN8/vOfj/PPPz8uuuii+Na3vhX79u2La665pt2jFe9LX/pSXH755XHWWWfFoUOH4m//9m9jcHAwNm3a1O7RijQ0NBS7d++e+nrv3r3x5JNPxtKlS+Oss86KG2+8MbZu3Rpr1qyJNWvWxNatW2PBggVx1VVXtXHqsrzea7B06dLYsmVLfPazn40VK1bE888/H1/5ylfijDPOiM985jNtnLoM1113Xdx9993xwx/+MPr6+qaO0CxZsiR6e3ujVqvZBubQGy3/oaGh+bv+t/EvtU7JP/7jP1arVq2quru7q4985CPT/jSNuXPllVdWK1asqLq6uqqBgYHqiiuuqHbu3NnusYr1k5/8pIqI11w2bdpUVdXxP4X96le/WvX391eNRqP6xCc+UT311FPtHbowr/caHDt2rNqwYUP1rne9q+rq6qrOOuusatOmTdW+ffvaPXYRTrTcI6K64447pu5jG5g7b7T85/P6X6uqqnoryxQAwFx62/3ODQDA61FuAICiKDcAQFGUGwCgKMoNAFAU5QYAKIpyAwAURbkBAIqi3AAARVFuAICiKDcAQFH+H5sianRxD0w2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grads are highest (black squares) at Yb index; trying to maximize\n",
    "# great pulley analogy of gradients being like a \"tug\" and all of the params feel the force and give into it\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.8079\n",
      "  10000/ 200000: 2.1585\n",
      "  20000/ 200000: 2.3531\n",
      "  30000/ 200000: 2.4037\n",
      "  40000/ 200000: 1.9873\n",
      "  50000/ 200000: 2.2946\n",
      "  60000/ 200000: 2.3924\n",
      "  70000/ 200000: 2.0233\n",
      "  80000/ 200000: 2.3813\n",
      "  90000/ 200000: 2.1565\n",
      " 100000/ 200000: 1.9760\n",
      " 110000/ 200000: 2.3433\n",
      " 120000/ 200000: 2.0446\n",
      " 130000/ 200000: 2.4298\n",
      " 140000/ 200000: 2.4106\n",
      " 150000/ 200000: 2.1319\n",
      " 160000/ 200000: 1.9271\n",
      " 170000/ 200000: 1.8624\n",
      " 180000/ 200000: 2.0249\n",
      " 190000/ 200000: 1.8852\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    # YOUR CODE HERE :)\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0707004070281982\n",
      "val 2.110710620880127\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahza.\n",
      "jahmarik.\n",
      "mri.\n",
      "reetlanna.\n",
      "sane.\n",
      "mahnen.\n",
      "delynn.\n",
      "jareei.\n",
      "nellara.\n",
      "chaiivia.\n",
      "leigh.\n",
      "ham.\n",
      "joce.\n",
      "quint.\n",
      "shoilea.\n",
      "jadiquintero.\n",
      "dearisi.\n",
      "jace.\n",
      "pirrat.\n",
      "edde.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "696138aadfbb39da019b0f4a82f739b9650213a50720b5fab5db3742a6f6c84f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
