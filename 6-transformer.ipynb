{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple decoder-only transformer\n",
    "\n",
    "- only need decoder because we are generating from data and only using self-attention (i.e. \"babbling\" Shakespeare), instead of feeding in key-value pairs from the encoder (i.e. translating a French sentence into an English sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-17 18:46:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  3.68MB/s    in 0.3s    \n",
      "\n",
      "2023-01-17 18:46:24 (3.68 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa47169c450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
    "  text = f.read()\n",
    "len(text) # number of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# vocab\n",
    "chars = sorted(list(set(text))) # get all unique chars in data\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# map chars and ints\n",
    "stoi = {ch:i for i,ch in enumerate(chars)} # dict\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: string -> list of int\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: list of int -> string\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# encode entire dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "print(x)\n",
    "y = train_data[1:block_size+1]\n",
    "print(y)\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # how many indepent sequences processed in parallel\n",
    "block_size = 8 # maximum context length for preds\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  ix = torch.randint(len(data)-block_size, (batch_size,)) # get batch_size number of random data values\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix]) # (4,8)\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # (4,8)\n",
    "  return x,y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension; same loop as before but add B\n",
    "  for t in range(block_size): # time dimension\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Baseline] Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # contains vocab_size number of tensors or size vocab_size (embedding is same length as vocab - what happens if we made this smaller? It should still work just lower dim embed space)\n",
    "  \n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx and targets are both (B,T)\n",
    "    logits = self.token_embedding_table(idx) # (B,T,C) where C is the second vocab_size above (additional dim we get from embedding)\n",
    "    \n",
    "    if targets is None: # during generation\n",
    "      loss = None\n",
    "    else:\n",
    "      B,T,C = logits.shape\n",
    "      logits = logits.view(B*T,C) # stack the batches on top of eachother; just some data wrangling for F.cross_entropy\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self(idx) # calls self.forward\n",
    "      logits = logits[:,-1,:] # (B,C); use -1 to get the last time step, AKA what does the last character think is most \"interesting\" from self-attn?\n",
    "      probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "      idx = torch.cat((idx,idx_next),dim=1) # (B,T+1) append to running sequence\n",
    "    \n",
    "    return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "# m.token_embedding_table(torch.tensor(encode(\"h\"))) # plucks out the row for \"h\" and does this for each letter in input\n",
    "logits, loss = m(xb,yb)\n",
    "# print(logits[0])\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0251, -1.6902,  0.7172],\n",
      "         [-0.6431,  0.0748,  0.6969],\n",
      "         [ 1.4970,  1.3448, -0.9685],\n",
      "         [-0.3677, -2.7265, -0.1685]],\n",
      "\n",
      "        [[ 1.4970,  1.3448, -0.9685],\n",
      "         [ 0.4362, -0.4004,  0.9400],\n",
      "         [-0.6431,  0.0748,  0.6969],\n",
      "         [ 0.9124, -2.3616,  1.1151]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0251, -1.6902,  0.7172],\n",
       "        [-0.6431,  0.0748,  0.6969],\n",
       "        [ 1.4970,  1.3448, -0.9685],\n",
       "        [-0.3677, -2.7265, -0.1685],\n",
       "        [ 1.4970,  1.3448, -0.9685],\n",
       "        [ 0.4362, -0.4004,  0.9400],\n",
       "        [-0.6431,  0.0748,  0.6969],\n",
       "        [ 0.9124, -2.3616,  1.1151]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing stuff\n",
    "a = torch.tensor([[[-0.0251, -1.6902,  0.7172],\n",
    "         [-0.6431,  0.0748,  0.6969],\n",
    "         [ 1.4970,  1.3448, -0.9685],\n",
    "         [-0.3677, -2.7265, -0.1685]],\n",
    "\n",
    "        [[ 1.4970,  1.3448, -0.9685],\n",
    "         [ 0.4362, -0.4004,  0.9400],\n",
    "         [-0.6431,  0.0748,  0.6969],\n",
    "         [ 0.9124, -2.3616,  1.1151]]])\n",
    "print(a)\n",
    "a.view(8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65630578994751\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(100):\n",
    "  xb,yb = get_batch(\"train\")\n",
    "  logits,loss = m(xb,yb)\n",
    "  optimizer.zero_grad(True) # zero out grads before loss (so it doesn't accumulate during training)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding_table.weight torch.Size([65, 65])\n"
     ]
    }
   ],
   "source": [
    "for name, param in m.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    print(name, param.data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention\n",
    "\n",
    "- Think of B,T,C as B by T matrix of tokens, each token with C dims\n",
    "- Self-attention insight is using non-uniform weights in tril matrix; some info is more \"interesting\" than others, and it is gathered in an independent way through KQV\n",
    "  - Information that makes up x, which is what K and Q operate on, is token and pos embedding\n",
    "  - X is kind of like private information; if you find anything interesting from me, I will communicate V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: use Softmax\n",
    "# aggregation is just mean of previous tokens\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # masking so only previous tokens in context are used\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # for ones that we don't want to see, weight it -inf\n",
    "wei = F.softmax(wei, dim=-1) # e^-inf = 0 -> same effect as taking average\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention\n",
    "# aggregation is based on attention mechanism with KQV insetad of simple mean\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# each node produces a key and a query\n",
    "k = key(x)  # (B,T,C) @ (C,16) = (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "\n",
    "# weight is based on how aligned key and query are; communicate by dot product queries and keys\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T); matmul for higher D just operates on last 2D\n",
    "\n",
    "# similar to version 3\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "print(out.shape)\n",
    "print(wei[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention heads\n",
    "\n",
    "- Head size is like how much info you \"communicate\" to other nodes\n",
    "  - Changes dimension of key and query channel\n",
    "- Having multiple attention heads is like having multiple communication channels, with different information\n",
    "  - More heads w/ smaller channels > 1 head w/ larger channels\n",
    "  - Can gather a lot of diff types of information\n",
    "  - Similar to having more convolutional filters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d); LayerNorm used more in LM because you don't know how long each input will be (aka how many tokens in sentence)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True)\n",
    "    xvar = x.var(1, keepdim=True)\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.5367e-09, -2.3842e-09, -2.0266e-08,  1.7881e-08,  1.6689e-08,\n",
       "         9.8348e-09,  4.7684e-09,  1.9073e-08, -1.4305e-08, -4.7684e-09,\n",
       "        -1.3113e-08, -5.9605e-09,  0.0000e+00, -7.1526e-09, -2.0266e-08,\n",
       "         7.0035e-09, -1.2815e-08,  1.7881e-08,  6.5565e-09, -4.7684e-09,\n",
       "         9.5367e-09, -3.5763e-09, -2.8610e-08,  4.7684e-09,  3.5763e-09,\n",
       "        -7.1526e-09, -4.7684e-09,  0.0000e+00,  5.3644e-09, -1.1921e-08,\n",
       "         4.7684e-09,  1.9073e-08])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(1) # across the 100 dimensions instead of batch of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.var(1) # creates mean of 0 and variance of 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of how transformers work:\n",
    "\n",
    "Blocks (Self attention + Feed forward -> Communication + Computation)\n",
    "- Each block has self attention + feedforward, and stacked together blocks create the transformer\n",
    "- Self attention is on a per token level, and then feed forward is \"thinking\" on the data that they gathered individually\n",
    "\n",
    "Add + Norm (Residual connnections and layer norm for optimizing larger networks)\n",
    "- There is a gradient flow of information, which is automatically passed through; each block is free to fork off, do some computation, and then project back onto residual pathway and add info to the flow\n",
    "  - Allows gradient to flow through to the last layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "696138aadfbb39da019b0f4a82f739b9650213a50720b5fab5db3742a6f6c84f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
